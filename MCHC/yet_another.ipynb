{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9282b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m new project at `C:\\Users\\79021\\AppData\\Local\\Temp\\jl_kfWMOH`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\79021\\.julia\\registries\\General.toml`\n",
      "\u001b[36m\u001b[1m     Project\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\79021\\AppData\\Local\\Temp\\jl_kfWMOH\\Project.toml`\n",
      "\u001b[36m\u001b[1m    Manifest\u001b[22m\u001b[39m No packages added to or removed from `C:\\Users\\79021\\AppData\\Local\\Temp\\jl_kfWMOH\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\79021\\AppData\\Local\\Temp\\jl_kfWMOH\\Project.toml`\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.16.7\u001b[39m\n",
      "  \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.4.7\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.41.4\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra v1.12.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\79021\\AppData\\Local\\Temp\\jl_kfWMOH\\Manifest.toml`\n",
      "  \u001b[90m[621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.5.0\u001b[39m\n",
      "  \u001b[90m[7d9f7c33] \u001b[39m\u001b[92m+ Accessors v0.1.43\u001b[39m\n",
      "  \u001b[90m[79e6a3ab] \u001b[39m\u001b[92m+ Adapt v4.4.0\u001b[39m\n",
      "  \u001b[90m[66dad0bd] \u001b[39m\u001b[92m+ AliasTables v1.1.3\u001b[39m\n",
      "  \u001b[90m[dce04be8] \u001b[39m\u001b[92m+ ArgCheck v2.5.0\u001b[39m\n",
      "  \u001b[90m[a9b6321e] \u001b[39m\u001b[92m+ Atomix v1.1.2\u001b[39m\n",
      "  \u001b[90m[198e06fe] \u001b[39m\u001b[92m+ BangBang v0.4.6\u001b[39m\n",
      "  \u001b[90m[9718e550] \u001b[39m\u001b[92m+ Baselet v0.1.1\u001b[39m\n",
      "  \u001b[90m[d1d4a3ce] \u001b[39m\u001b[92m+ BitFlags v0.1.9\u001b[39m\n",
      "  \u001b[90m[082447d4] \u001b[39m\u001b[92m+ ChainRules v1.72.6\u001b[39m\n",
      "  \u001b[90m[d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v1.26.0\u001b[39m\n",
      "  \u001b[90m[944b1d66] \u001b[39m\u001b[92m+ CodecZlib v0.7.8\u001b[39m\n",
      "  \u001b[90m[35d6a980] \u001b[39m\u001b[92m+ ColorSchemes v3.31.0\u001b[39m\n",
      "  \u001b[90m[3da002f7] \u001b[39m\u001b[92m+ ColorTypes v0.12.1\u001b[39m\n",
      "  \u001b[90m[c3611d14] \u001b[39m\u001b[92m+ ColorVectorSpace v0.11.0\u001b[39m\n",
      "  \u001b[90m[5ae59095] \u001b[39m\u001b[92m+ Colors v0.13.1\u001b[39m\n",
      "  \u001b[90m[bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.1\u001b[39m\n",
      "  \u001b[90m[34da2185] \u001b[39m\u001b[92m+ Compat v4.18.1\u001b[39m\n",
      "  \u001b[90m[a33af91c] \u001b[39m\u001b[92m+ CompositionsBase v0.1.2\u001b[39m\n",
      "  \u001b[90m[f0e56b4a] \u001b[39m\u001b[92m+ ConcurrentUtilities v2.5.0\u001b[39m\n",
      "  \u001b[90m[187b0558] \u001b[39m\u001b[92m+ ConstructionBase v1.6.0\u001b[39m\n",
      "  \u001b[90m[6add18c4] \u001b[39m\u001b[92m+ ContextVariablesX v0.1.3\u001b[39m\n",
      "  \u001b[90m[d38c429a] \u001b[39m\u001b[92m+ Contour v0.6.3\u001b[39m\n",
      "  \u001b[90m[9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.16.0\u001b[39m\n",
      "  \u001b[90m[864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.19.3\u001b[39m\n",
      "  \u001b[90m[e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n",
      "  \u001b[90m[244e2a9f] \u001b[39m\u001b[92m+ DefineSingletons v0.1.2\u001b[39m\n",
      "  \u001b[90m[8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles v1.9.1\u001b[39m\n",
      "  \u001b[90m[163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.1.0\u001b[39m\n",
      "  \u001b[90m[b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.15.1\u001b[39m\n",
      "  \u001b[90m[ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.5\u001b[39m\n",
      "  \u001b[90m[f151be2c] \u001b[39m\u001b[92m+ EnzymeCore v0.8.18\u001b[39m\n",
      "  \u001b[90m[460bff9d] \u001b[39m\u001b[92m+ ExceptionUnwrapping v0.1.11\u001b[39m\n",
      "  \u001b[90m[c87230d0] \u001b[39m\u001b[92m+ FFMPEG v0.4.5\u001b[39m\n",
      "  \u001b[90m[cc61a311] \u001b[39m\u001b[92m+ FLoops v0.2.2\u001b[39m\n",
      "  \u001b[90m[b9860ae5] \u001b[39m\u001b[92m+ FLoopsBase v0.1.1\u001b[39m\n",
      "  \u001b[90m[1a297f60] \u001b[39m\u001b[92m+ FillArrays v1.16.0\u001b[39m\n",
      "  \u001b[90m[53c48c17] \u001b[39m\u001b[92m+ FixedPointNumbers v0.8.5\u001b[39m\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.16.7\u001b[39m\n",
      "  \u001b[90m[1fa38f19] \u001b[39m\u001b[92m+ Format v1.3.7\u001b[39m\n",
      "  \u001b[90m[f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v1.3.1\u001b[39m\n",
      "  \u001b[90m[d9f16b24] \u001b[39m\u001b[92m+ Functors v0.5.2\u001b[39m\n",
      "  \u001b[90m[46192b85] \u001b[39m\u001b[92m+ GPUArraysCore v0.2.0\u001b[39m\n",
      "  \u001b[90m[28b8d3ca] \u001b[39m\u001b[92m+ GR v0.73.19\u001b[39m\n",
      "  \u001b[90m[42e2da0e] \u001b[39m\u001b[92m+ Grisu v1.0.2\u001b[39m\n",
      "  \u001b[90m[cd3eb016] \u001b[39m\u001b[92m+ HTTP v1.10.19\u001b[39m\n",
      "  \u001b[90m[076d061b] \u001b[39m\u001b[92m+ HashArrayMappedTries v0.2.0\u001b[39m\n",
      "  \u001b[90m[7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.15\u001b[39m\n",
      "  \u001b[90m[22cec73e] \u001b[39m\u001b[92m+ InitialValues v0.3.1\u001b[39m\n",
      "  \u001b[90m[3587e190] \u001b[39m\u001b[92m+ InverseFunctions v0.1.17\u001b[39m\n",
      "  \u001b[90m[92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.2.6\u001b[39m\n",
      "  \u001b[90m[82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
      "  \u001b[90m[1019f520] \u001b[39m\u001b[92m+ JLFzf v0.1.11\u001b[39m\n",
      "  \u001b[90m[692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.7.1\u001b[39m\n",
      "  \u001b[90m[682c06a0] \u001b[39m\u001b[92m+ JSON v1.4.0\u001b[39m\n",
      "  \u001b[90m[b14d175d] \u001b[39m\u001b[92m+ JuliaVariables v0.2.4\u001b[39m\n",
      "  \u001b[90m[63c18a36] \u001b[39m\u001b[92m+ KernelAbstractions v0.9.39\u001b[39m\n",
      "  \u001b[90m[b964fa9f] \u001b[39m\u001b[92m+ LaTeXStrings v1.4.0\u001b[39m\n",
      "  \u001b[90m[23fbe1c1] \u001b[39m\u001b[92m+ Latexify v0.16.10\u001b[39m\n",
      "  \u001b[90m[2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.29\u001b[39m\n",
      "  \u001b[90m[e6f89c97] \u001b[39m\u001b[92m+ LoggingExtras v1.2.0\u001b[39m\n",
      "  \u001b[90m[c2834f40] \u001b[39m\u001b[92m+ MLCore v1.0.0\u001b[39m\n",
      "  \u001b[90m[7e8f7934] \u001b[39m\u001b[92m+ MLDataDevices v1.17.1\u001b[39m\n",
      "  \u001b[90m[d8e11817] \u001b[39m\u001b[92m+ MLStyle v0.4.17\u001b[39m\n",
      "  \u001b[90m[f1d291b0] \u001b[39m\u001b[92m+ MLUtils v0.4.8\u001b[39m\n",
      "  \u001b[90m[1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.16\u001b[39m\n",
      "  \u001b[90m[739be429] \u001b[39m\u001b[92m+ MbedTLS v1.1.9\u001b[39m\n",
      "  \u001b[90m[442fdcdd] \u001b[39m\u001b[92m+ Measures v0.3.3\u001b[39m\n",
      "  \u001b[90m[128add7d] \u001b[39m\u001b[92m+ MicroCollections v0.2.0\u001b[39m\n",
      "  \u001b[90m[e1d29d7a] \u001b[39m\u001b[92m+ Missings v1.2.0\u001b[39m\n",
      "  \u001b[90m[872c559c] \u001b[39m\u001b[92m+ NNlib v0.9.33\u001b[39m\n",
      "  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.1.3\u001b[39m\n",
      "  \u001b[90m[71a1bf82] \u001b[39m\u001b[92m+ NameResolution v0.1.5\u001b[39m\n",
      "  \u001b[90m[0b1bfda6] \u001b[39m\u001b[92m+ OneHotArrays v0.2.10\u001b[39m\n",
      "  \u001b[90m[4d8831e6] \u001b[39m\u001b[92m+ OpenSSL v1.6.1\u001b[39m\n",
      "  \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.4.7\u001b[39m\n",
      "  \u001b[90m[bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.8.1\u001b[39m\n",
      "  \u001b[90m[69de0a69] \u001b[39m\u001b[92m+ Parsers v2.8.3\u001b[39m\n",
      "  \u001b[90m[ccf2f8ad] \u001b[39m\u001b[92m+ PlotThemes v3.3.0\u001b[39m\n",
      "  \u001b[90m[995b91a9] \u001b[39m\u001b[92m+ PlotUtils v1.4.4\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.41.4\u001b[39m\n",
      "  \u001b[90m[aea7be01] \u001b[39m\u001b[92m+ PrecompileTools v1.3.3\u001b[39m\n",
      "  \u001b[90m[21216c6a] \u001b[39m\u001b[92m+ Preferences v1.5.1\u001b[39m\n",
      "  \u001b[90m[8162dcfd] \u001b[39m\u001b[92m+ PrettyPrint v0.2.0\u001b[39m\n",
      "  \u001b[90m[33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.6\u001b[39m\n",
      "  \u001b[90m[43287f4e] \u001b[39m\u001b[92m+ PtrArrays v1.3.0\u001b[39m\n",
      "  \u001b[90m[c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      "  \u001b[90m[3cdcf5f2] \u001b[39m\u001b[92m+ RecipesBase v1.3.4\u001b[39m\n",
      "  \u001b[90m[01d81517] \u001b[39m\u001b[92m+ RecipesPipeline v0.6.12\u001b[39m\n",
      "  \u001b[90m[189a3867] \u001b[39m\u001b[92m+ Reexport v1.2.2\u001b[39m\n",
      "  \u001b[90m[05181044] \u001b[39m\u001b[92m+ RelocatableFolders v1.0.1\u001b[39m\n",
      "  \u001b[90m[ae029012] \u001b[39m\u001b[92m+ Requires v1.3.1\u001b[39m\n",
      "  \u001b[90m[431bcebd] \u001b[39m\u001b[92m+ SciMLPublic v1.0.1\u001b[39m\n",
      "  \u001b[90m[7e506255] \u001b[39m\u001b[92m+ ScopedValues v1.5.0\u001b[39m\n",
      "  \u001b[90m[6c6a2e73] \u001b[39m\u001b[92m+ Scratch v1.3.0\u001b[39m\n",
      "  \u001b[90m[efcf1570] \u001b[39m\u001b[92m+ Setfield v1.1.2\u001b[39m\n",
      "  \u001b[90m[605ecd9f] \u001b[39m\u001b[92m+ ShowCases v0.1.0\u001b[39m\n",
      "  \u001b[90m[992d4aef] \u001b[39m\u001b[92m+ Showoff v1.0.3\u001b[39m\n",
      "  \u001b[90m[777ac1f9] \u001b[39m\u001b[92m+ SimpleBufferStream v1.2.0\u001b[39m\n",
      "  \u001b[90m[699a6c99] \u001b[39m\u001b[92m+ SimpleTraits v0.9.5\u001b[39m\n",
      "  \u001b[90m[a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v1.2.2\u001b[39m\n",
      "  \u001b[90m[dc90abb0] \u001b[39m\u001b[92m+ SparseInverseSubset v0.1.2\u001b[39m\n",
      "  \u001b[90m[276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v2.6.1\u001b[39m\n",
      "  \u001b[90m[171d559e] \u001b[39m\u001b[92m+ SplittablesBase v0.1.15\u001b[39m\n",
      "  \u001b[90m[860ef19b] \u001b[39m\u001b[92m+ StableRNGs v1.0.4\u001b[39m\n",
      "  \u001b[90m[90137ffa] \u001b[39m\u001b[92m+ StaticArrays v1.9.16\u001b[39m\n",
      "  \u001b[90m[1e83bf80] \u001b[39m\u001b[92m+ StaticArraysCore v1.4.4\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.8.0\u001b[39m\n",
      "  \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.34.10\u001b[39m\n",
      "  \u001b[90m[09ab397b] \u001b[39m\u001b[92m+ StructArrays v0.7.2\u001b[39m\n",
      "  \u001b[90m[ec057cc2] \u001b[39m\u001b[92m+ StructUtils v2.6.2\u001b[39m\n",
      "  \u001b[90m[3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.1\u001b[39m\n",
      "  \u001b[90m[bd369af6] \u001b[39m\u001b[92m+ Tables v1.12.1\u001b[39m\n",
      "  \u001b[90m[62fd8b95] \u001b[39m\u001b[92m+ TensorCore v0.1.1\u001b[39m\n",
      "  \u001b[90m[3bb67fe8] \u001b[39m\u001b[92m+ TranscodingStreams v0.11.3\u001b[39m\n",
      "  \u001b[90m[28d57a85] \u001b[39m\u001b[92m+ Transducers v0.4.85\u001b[39m\n",
      "  \u001b[90m[5c2747f8] \u001b[39m\u001b[92m+ URIs v1.6.1\u001b[39m\n",
      "  \u001b[90m[1cfade01] \u001b[39m\u001b[92m+ UnicodeFun v0.4.1\u001b[39m\n",
      "  \u001b[90m[013be700] \u001b[39m\u001b[92m+ UnsafeAtomics v0.3.0\u001b[39m\n",
      "  \u001b[90m[41fe7b60] \u001b[39m\u001b[92m+ Unzip v0.2.0\u001b[39m\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.7.10\u001b[39m\n",
      "  \u001b[90m[700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.7\u001b[39m\n",
      "  \u001b[90m[6e34b625] \u001b[39m\u001b[92m+ Bzip2_jll v1.0.9+0\u001b[39m\n",
      "  \u001b[90m[83423d85] \u001b[39m\u001b[92m+ Cairo_jll v1.18.5+0\u001b[39m\n",
      "  \u001b[90m[ee1fde0b] \u001b[39m\u001b[92m+ Dbus_jll v1.16.2+0\u001b[39m\n",
      "  \u001b[90m[2702e6a9] \u001b[39m\u001b[92m+ EpollShim_jll v0.0.20230411+1\u001b[39m\n",
      "  \u001b[90m[2e619515] \u001b[39m\u001b[92m+ Expat_jll v2.7.3+0\u001b[39m\n",
      "  \u001b[90m[b22a6f82] \u001b[39m\u001b[92m+ FFMPEG_jll v8.0.1+0\u001b[39m\n",
      "  \u001b[90m[a3f928ae] \u001b[39m\u001b[92m+ Fontconfig_jll v2.17.1+0\u001b[39m\n",
      "  \u001b[90m[d7e528f0] \u001b[39m\u001b[92m+ FreeType2_jll v2.13.4+0\u001b[39m\n",
      "  \u001b[90m[559328eb] \u001b[39m\u001b[92m+ FriBidi_jll v1.0.17+0\u001b[39m\n",
      "  \u001b[90m[0656b61e] \u001b[39m\u001b[92m+ GLFW_jll v3.4.1+0\u001b[39m\n",
      "  \u001b[90m[d2c73de3] \u001b[39m\u001b[92m+ GR_jll v0.73.19+1\u001b[39m\n",
      "  \u001b[90m[b0724c58] \u001b[39m\u001b[92m+ GettextRuntime_jll v0.22.4+0\u001b[39m\n",
      "  \u001b[90m[61579ee1] \u001b[39m\u001b[92m+ Ghostscript_jll v9.55.1+0\u001b[39m\n",
      "  \u001b[90m[7746bdde] \u001b[39m\u001b[92m+ Glib_jll v2.86.2+0\u001b[39m\n",
      "  \u001b[90m[3b182d85] \u001b[39m\u001b[92m+ Graphite2_jll v1.3.15+0\u001b[39m\n",
      "  \u001b[90m[2e76f6c2] \u001b[39m\u001b[92m+ HarfBuzz_jll v8.5.1+0\u001b[39m\n",
      "  \u001b[90m[aacddb02] \u001b[39m\u001b[92m+ JpegTurbo_jll v3.1.4+0\u001b[39m\n",
      "  \u001b[90m[c1c5ebd0] \u001b[39m\u001b[92m+ LAME_jll v3.100.3+0\u001b[39m\n",
      "  \u001b[90m[88015f11] \u001b[39m\u001b[92m+ LERC_jll v4.0.1+0\u001b[39m\n",
      "  \u001b[90m[1d63c593] \u001b[39m\u001b[92m+ LLVMOpenMP_jll v18.1.8+0\u001b[39m\n",
      "  \u001b[90m[dd4b983a] \u001b[39m\u001b[92m+ LZO_jll v2.10.3+0\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[e9f186c6] \u001b[39m\u001b[92m+ Libffi_jll v3.4.7+0\u001b[39m\n",
      "  \u001b[90m[7e76a0d4] \u001b[39m\u001b[92m+ Libglvnd_jll v1.7.1+1\u001b[39m\n",
      "  \u001b[90m[94ce4f54] \u001b[39m\u001b[92m+ Libiconv_jll v1.18.0+0\u001b[39m\n",
      "  \u001b[90m[4b2f31a3] \u001b[39m\u001b[92m+ Libmount_jll v2.41.2+0\u001b[39m\n",
      "  \u001b[90m[89763e89] \u001b[39m\u001b[92m+ Libtiff_jll v4.7.2+0\u001b[39m\n",
      "  \u001b[90m[38a345b3] \u001b[39m\u001b[92m+ Libuuid_jll v2.41.2+0\u001b[39m\n",
      "  \u001b[90m[c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.28.1010+0\u001b[39m\n",
      "  \u001b[90m[e7412a2a] \u001b[39m\u001b[92m+ Ogg_jll v1.3.6+0\u001b[39m\n",
      "  \u001b[90m[efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.6+0\u001b[39m\n",
      "  \u001b[90m[91d4177d] \u001b[39m\u001b[92m+ Opus_jll v1.6.0+0\u001b[39m\n",
      "  \u001b[90m[36c8627f] \u001b[39m\u001b[92m+ Pango_jll v1.57.0+0\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[30392449] \u001b[39m\u001b[92m+ Pixman_jll v0.44.2+0\u001b[39m\n",
      "  \u001b[90m[c0090381] \u001b[39m\u001b[92m+ Qt6Base_jll v6.8.2+2\u001b[39m\n",
      "  \u001b[90m[629bc702] \u001b[39m\u001b[92m+ Qt6Declarative_jll v6.8.2+1\u001b[39m\n",
      "  \u001b[90m[ce943373] \u001b[39m\u001b[92m+ Qt6ShaderTools_jll v6.8.2+1\u001b[39m\n",
      "  \u001b[90m[e99dba38] \u001b[39m\u001b[92m+ Qt6Wayland_jll v6.8.2+2\u001b[39m\n",
      "  \u001b[90m[a44049a8] \u001b[39m\u001b[92m+ Vulkan_Loader_jll v1.3.243+0\u001b[39m\n",
      "  \u001b[90m[a2964d1f] \u001b[39m\u001b[92m+ Wayland_jll v1.24.0+0\u001b[39m\n",
      "  \u001b[90m[ffd25f8a] \u001b[39m\u001b[92m+ XZ_jll v5.8.2+0\u001b[39m\n",
      "  \u001b[90m[f67eecfb] \u001b[39m\u001b[92m+ Xorg_libICE_jll v1.1.2+0\u001b[39m\n",
      "  \u001b[90m[c834827a] \u001b[39m\u001b[92m+ Xorg_libSM_jll v1.2.6+0\u001b[39m\n",
      "  \u001b[90m[4f6342f7] \u001b[39m\u001b[92m+ Xorg_libX11_jll v1.8.12+0\u001b[39m\n",
      "  \u001b[90m[0c0b7dd1] \u001b[39m\u001b[92m+ Xorg_libXau_jll v1.0.13+0\u001b[39m\n",
      "  \u001b[90m[935fb764] \u001b[39m\u001b[92m+ Xorg_libXcursor_jll v1.2.4+0\u001b[39m\n",
      "  \u001b[90m[a3789734] \u001b[39m\u001b[92m+ Xorg_libXdmcp_jll v1.1.6+0\u001b[39m\n",
      "  \u001b[90m[1082639a] \u001b[39m\u001b[92m+ Xorg_libXext_jll v1.3.7+0\u001b[39m\n",
      "  \u001b[90m[d091e8ba] \u001b[39m\u001b[92m+ Xorg_libXfixes_jll v6.0.2+0\u001b[39m\n",
      "  \u001b[90m[a51aa0fd] \u001b[39m\u001b[92m+ Xorg_libXi_jll v1.8.3+0\u001b[39m\n",
      "  \u001b[90m[d1454406] \u001b[39m\u001b[92m+ Xorg_libXinerama_jll v1.1.6+0\u001b[39m\n",
      "  \u001b[90m[ec84b674] \u001b[39m\u001b[92m+ Xorg_libXrandr_jll v1.5.5+0\u001b[39m\n",
      "  \u001b[90m[ea2f1a96] \u001b[39m\u001b[92m+ Xorg_libXrender_jll v0.9.12+0\u001b[39m\n",
      "  \u001b[90m[c7cfdc94] \u001b[39m\u001b[92m+ Xorg_libxcb_jll v1.17.1+0\u001b[39m\n",
      "  \u001b[90m[cc61e674] \u001b[39m\u001b[92m+ Xorg_libxkbfile_jll v1.1.3+0\u001b[39m\n",
      "  \u001b[90m[e920d4aa] \u001b[39m\u001b[92m+ Xorg_xcb_util_cursor_jll v0.1.6+0\u001b[39m\n",
      "  \u001b[90m[12413925] \u001b[39m\u001b[92m+ Xorg_xcb_util_image_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[2def613f] \u001b[39m\u001b[92m+ Xorg_xcb_util_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[975044d2] \u001b[39m\u001b[92m+ Xorg_xcb_util_keysyms_jll v0.4.1+0\u001b[39m\n",
      "  \u001b[90m[0d47668e] \u001b[39m\u001b[92m+ Xorg_xcb_util_renderutil_jll v0.3.10+0\u001b[39m\n",
      "  \u001b[90m[c22f9ab0] \u001b[39m\u001b[92m+ Xorg_xcb_util_wm_jll v0.4.2+0\u001b[39m\n",
      "  \u001b[90m[35661453] \u001b[39m\u001b[92m+ Xorg_xkbcomp_jll v1.4.7+0\u001b[39m\n",
      "  \u001b[90m[33bec58e] \u001b[39m\u001b[92m+ Xorg_xkeyboard_config_jll v2.44.0+0\u001b[39m\n",
      "  \u001b[90m[c5fb5394] \u001b[39m\u001b[92m+ Xorg_xtrans_jll v1.6.0+0\u001b[39m\n",
      "  \u001b[90m[3161d3a3] \u001b[39m\u001b[92m+ Zstd_jll v1.5.7+1\u001b[39m\n",
      "  \u001b[90m[35ca27e7] \u001b[39m\u001b[92m+ eudev_jll v3.2.14+0\u001b[39m\n",
      "  \u001b[90m[214eeab7] \u001b[39m\u001b[92m+ fzf_jll v0.61.1+0\u001b[39m\n",
      "  \u001b[90m[a4ae2306] \u001b[39m\u001b[92m+ libaom_jll v3.13.1+0\u001b[39m\n",
      "  \u001b[90m[0ac62f75] \u001b[39m\u001b[92m+ libass_jll v0.17.4+0\u001b[39m\n",
      "  \u001b[90m[1183f4f0] \u001b[39m\u001b[92m+ libdecor_jll v0.2.2+0\u001b[39m\n",
      "  \u001b[90m[2db6ffa8] \u001b[39m\u001b[92m+ libevdev_jll v1.13.4+0\u001b[39m\n",
      "  \u001b[90m[f638f0a6] \u001b[39m\u001b[92m+ libfdk_aac_jll v2.0.4+0\u001b[39m\n",
      "  \u001b[90m[36db933b] \u001b[39m\u001b[92m+ libinput_jll v1.28.1+0\u001b[39m\n",
      "  \u001b[90m[b53b4c65] \u001b[39m\u001b[92m+ libpng_jll v1.6.54+0\u001b[39m\n",
      "  \u001b[90m[f27f6e37] \u001b[39m\u001b[92m+ libvorbis_jll v1.3.8+0\u001b[39m\n",
      "  \u001b[90m[009596ad] \u001b[39m\u001b[92m+ mtdev_jll v1.1.7+0\u001b[39m\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[1270edf5] \u001b[39m\u001b[92m+ x264_jll v10164.0.1+0\u001b[39m\n",
      "  \u001b[90m[dfaa095f] \u001b[39m\u001b[92m+ x265_jll v4.1.0+0\u001b[39m\n",
      "  \u001b[90m[d8fb68d0] \u001b[39m\u001b[92m+ xkbcommon_jll v1.13.0+0\u001b[39m\n",
      "  \u001b[90m[0dad84c5] \u001b[39m\u001b[92m+ ArgTools v1.1.2\u001b[39m\n",
      "  \u001b[90m[56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.11.0\u001b[39m\n",
      "  \u001b[90m[2a0f44e3] \u001b[39m\u001b[92m+ Base64 v1.11.0\u001b[39m\n",
      "  \u001b[90m[ade2ca70] \u001b[39m\u001b[92m+ Dates v1.11.0\u001b[39m\n",
      "  \u001b[90m[8ba89e20] \u001b[39m\u001b[92m+ Distributed v1.11.0\u001b[39m\n",
      "  \u001b[90m[f43a241f] \u001b[39m\u001b[92m+ Downloads v1.7.0\u001b[39m\n",
      "  \u001b[90m[7b1f6079] \u001b[39m\u001b[92m+ FileWatching v1.11.0\u001b[39m\n",
      "  \u001b[90m[9fa8497b] \u001b[39m\u001b[92m+ Future v1.11.0\u001b[39m\n",
      "  \u001b[90m[b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils v1.11.0\u001b[39m\n",
      "  \u001b[90m[ac6e5ff7] \u001b[39m\u001b[92m+ JuliaSyntaxHighlighting v1.12.0\u001b[39m\n",
      "  \u001b[90m[b27032c2] \u001b[39m\u001b[92m+ LibCURL v0.6.4\u001b[39m\n",
      "  \u001b[90m[76f85450] \u001b[39m\u001b[92m+ LibGit2 v1.11.0\u001b[39m\n",
      "  \u001b[90m[8f399da3] \u001b[39m\u001b[92m+ Libdl v1.11.0\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra v1.12.0\u001b[39m\n",
      "  \u001b[90m[56ddb016] \u001b[39m\u001b[92m+ Logging v1.11.0\u001b[39m\n",
      "  \u001b[90m[d6f4376e] \u001b[39m\u001b[92m+ Markdown v1.11.0\u001b[39m\n",
      "  \u001b[90m[a63ad114] \u001b[39m\u001b[92m+ Mmap v1.11.0\u001b[39m\n",
      "  \u001b[90m[ca575930] \u001b[39m\u001b[92m+ NetworkOptions v1.3.0\u001b[39m\n",
      "  \u001b[90m[44cfe95a] \u001b[39m\u001b[92m+ Pkg v1.12.1\u001b[39m\n",
      "  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf v1.11.0\u001b[39m\n",
      "  \u001b[90m[3fa0cd96] \u001b[39m\u001b[92m+ REPL v1.11.0\u001b[39m\n",
      "  \u001b[90m[9a3f8284] \u001b[39m\u001b[92m+ Random v1.11.0\u001b[39m\n",
      "  \u001b[90m[ea8e919c] \u001b[39m\u001b[92m+ SHA v0.7.0\u001b[39m\n",
      "  \u001b[90m[9e88b42a] \u001b[39m\u001b[92m+ Serialization v1.11.0\u001b[39m\n",
      "  \u001b[90m[6462fe0b] \u001b[39m\u001b[92m+ Sockets v1.11.0\u001b[39m\n",
      "  \u001b[90m[2f01184e] \u001b[39m\u001b[92m+ SparseArrays v1.12.0\u001b[39m\n",
      "  \u001b[90m[f489334b] \u001b[39m\u001b[92m+ StyledStrings v1.11.0\u001b[39m\n",
      "  \u001b[90m[4607b0f0] \u001b[39m\u001b[92m+ SuiteSparse\u001b[39m\n",
      "  \u001b[90m[fa267f1f] \u001b[39m\u001b[92m+ TOML v1.0.3\u001b[39m\n",
      "  \u001b[90m[a4e569a6] \u001b[39m\u001b[92m+ Tar v1.10.0\u001b[39m\n",
      "  \u001b[90m[8dfed614] \u001b[39m\u001b[92m+ Test v1.11.0\u001b[39m\n",
      "  \u001b[90m[cf7118a7] \u001b[39m\u001b[92m+ UUIDs v1.11.0\u001b[39m\n",
      "  \u001b[90m[4ec0a83e] \u001b[39m\u001b[92m+ Unicode v1.11.0\u001b[39m\n",
      "  \u001b[90m[e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v1.3.0+1\u001b[39m\n",
      "  \u001b[90m[deac9b47] \u001b[39m\u001b[92m+ LibCURL_jll v8.15.0+0\u001b[39m\n",
      "  \u001b[90m[e37daf67] \u001b[39m\u001b[92m+ LibGit2_jll v1.9.0+0\u001b[39m\n",
      "  \u001b[90m[29816b5a] \u001b[39m\u001b[92m+ LibSSH2_jll v1.11.3+1\u001b[39m\n",
      "  \u001b[90m[14a3606d] \u001b[39m\u001b[92m+ MozillaCACerts_jll v2025.5.20\u001b[39m\n",
      "  \u001b[90m[4536629a] \u001b[39m\u001b[92m+ OpenBLAS_jll v0.3.29+0\u001b[39m\n",
      "  \u001b[90m[05823500] \u001b[39m\u001b[92m+ OpenLibm_jll v0.8.7+0\u001b[39m\n",
      "  \u001b[90m[458c3c95] \u001b[39m\u001b[92m+ OpenSSL_jll v3.5.4+0\u001b[39m\n",
      "  \u001b[90m[efcefdf7] \u001b[39m\u001b[92m+ PCRE2_jll v10.44.0+1\u001b[39m\n",
      "  \u001b[90m[bea87d4a] \u001b[39m\u001b[92m+ SuiteSparse_jll v7.8.3+2\u001b[39m\n",
      "  \u001b[90m[83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.3.1+2\u001b[39m\n",
      "  \u001b[90m[8e850b90] \u001b[39m\u001b[92m+ libblastrampoline_jll v5.15.0+0\u001b[39m\n",
      "  \u001b[90m[8e850ede] \u001b[39m\u001b[92m+ nghttp2_jll v1.64.0+1\u001b[39m\n",
      "  \u001b[90m[3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v17.7.0+0\u001b[39m\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[33m⌅\u001b[39m have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated -m`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(mktempdir())\n",
    "Pkg.update()\n",
    "Pkg.add([\n",
    "    \"Flux\",\n",
    "    \"LinearAlgebra\",\n",
    "    \"Statistics\",\n",
    "    \"Plots\",\n",
    "    \"Optimisers\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77f3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float32}:\n",
       " -0.84  -1.19   0.52   0.78\n",
       " -0.21  -0.3   -0.69   0.88\n",
       " -0.38  -0.8    1.76  -0.23\n",
       "  0.12  -0.19  -0.36  -0.45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float32}:\n",
       " 0.169  0.155  0.259  0.418\n",
       " 0.257  0.308  0.063  0.373\n",
       " 0.172  0.149  0.581  0.098\n",
       " 0.402  0.389  0.098  0.111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "mHC: Complete Demonstration\n",
      "============================================================\n",
      "==================================================\n",
      "DEMO: Sinkhorn-Knopp Projection\n",
      "==================================================\n",
      "\n",
      "Random matrix:\n",
      "\n",
      "\n",
      "After Sinkhorn-Knopp (doubly stochastic):\n",
      "\n",
      "Verification:\n",
      "Row sums: Float32[1.0, 1.0, 1.0, 1.0]\n",
      "Col sums: Float32[1.0, 1.0, 1.0, 1.0]\n",
      "All non-negative: true\n",
      "Doubly stochastic: true\n",
      "==================================================\n",
      "DEMO: Propagation Stability Comparison\n",
      "==================================================\n",
      "\n",
      "--- Unconstrained (HC-style) ---\n",
      "Single layer: forward=7.57, backward=7.05\n",
      "30-layer composite: forward=6.385931e8, backward=6.995837e8\n",
      "\n",
      "--- Doubly Stochastic (mHC-style) ---\n",
      "Single layer: forward=1.0, backward=1.0\n",
      "30-layer composite: forward=1.0, backward=1.0\n",
      "============================================================\n",
      "TRAINING COMPARISON: Residual vs HC vs mHC\n",
      "============================================================\n",
      "Layers: 8, Hidden: 64, Expansion: 4\n",
      "Data: (32, 3000)\n",
      "\n",
      "Creating models...\n",
      "ResidualMLP: 268618 params\n",
      "HCMLP: 324514 params"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# mHC: Manifold-Constrained Hyper-Connections\n",
    "# Jupyter Notebook Version\n",
    "# =============================================================================\n",
    "# Copy each section into a separate notebook cell\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 1: Package Imports\n",
    "# =============================================================================\n",
    "\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold\n",
    "using Optimisers: destructure\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Random\n",
    "using Plots\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: Sinkhorn-Knopp Algorithm\n",
    "# Projects matrix onto Birkhoff polytope (doubly stochastic matrices)\n",
    "# =============================================================================\n",
    "\n",
    "function sinkhorn_knopp(M::AbstractMatrix; max_iters::Int=20, ε::Float32=1f-8)\n",
    "    # Make all entries positive via exponentiation\n",
    "    # Maps ℝ → ℝ⁺, preserving differentiability\n",
    "    P = exp.(M)\n",
    "    \n",
    "    # Alternating normalisation converges to doubly stochastic\n",
    "    for _ in 1:max_iters\n",
    "        P = P ./ (sum(P, dims=2) .+ ε)  # Row normalisation\n",
    "        P = P ./ (sum(P, dims=1) .+ ε)  # Column normalisation\n",
    "    end\n",
    "    \n",
    "    return P\n",
    "end\n",
    "\n",
    "function check_doubly_stochastic(M::AbstractMatrix; tol::Float64=1e-3)\n",
    "    row_sums = vec(sum(M, dims=2))\n",
    "    col_sums = vec(sum(M, dims=1))\n",
    "    \n",
    "    row_ok = all(abs.(row_sums .- 1.0) .< tol)\n",
    "    col_ok = all(abs.(col_sums .- 1.0) .< tol)\n",
    "    non_neg = all(M .>= 0)\n",
    "    \n",
    "    println(\"Row sums: \", round.(row_sums, digits=4))\n",
    "    println(\"Col sums: \", round.(col_sums, digits=4))\n",
    "    println(\"All non-negative: \", non_neg)\n",
    "    println(\"Doubly stochastic: \", row_ok && col_ok && non_neg)\n",
    "    \n",
    "    return row_ok && col_ok && non_neg\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 3: Stability Measurement Utilities\n",
    "# =============================================================================\n",
    "\n",
    "function measure_gain(H::AbstractMatrix)\n",
    "    # Forward: signal x → H·x, gain is max row sum\n",
    "    forward_gain = maximum(sum(abs.(H), dims=2))\n",
    "    # Backward: gradient g → Hᵀ·g, gain is max column sum\n",
    "    backward_gain = maximum(sum(abs.(H), dims=1))\n",
    "    return (forward=forward_gain, backward=backward_gain)\n",
    "end\n",
    "\n",
    "function measure_composite_gain(matrices::Vector{<:AbstractMatrix})\n",
    "    composite = matrices[1]\n",
    "    for i in 2:length(matrices)\n",
    "        composite = matrices[i] * composite  # Fixed: was matrices[1]\n",
    "    end\n",
    "    return measure_gain(composite)\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 4: Standard Residual Block (Baseline)\n",
    "# x_{l+1} = x_l + F(x_l)\n",
    "# =============================================================================\n",
    "\n",
    "struct ResidualBlock\n",
    "    layer::Any\n",
    "end\n",
    "\n",
    "Flux.@layer ResidualBlock\n",
    "\n",
    "function (rb::ResidualBlock)(x)\n",
    "    return x .+ rb.layer(x)\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 5: Hyper-Connections Layer (Unstable)\n",
    "# Demonstrates the instability problem with unconstrained H_res\n",
    "# =============================================================================\n",
    "\n",
    "struct HyperConnectionLayer\n",
    "    n::Int\n",
    "    C::Int\n",
    "    layer::Any\n",
    "    \n",
    "    ϕ_pre::Any\n",
    "    ϕ_post::Any\n",
    "    ϕ_res::Any\n",
    "    \n",
    "    b_pre::Any\n",
    "    b_post::Any\n",
    "    b_res::Any\n",
    "    \n",
    "    α_pre::Any\n",
    "    α_post::Any\n",
    "    α_res::Any\n",
    "end\n",
    "\n",
    "Flux.@layer HyperConnectionLayer\n",
    "\n",
    "function HyperConnectionLayer(n::Int, C::Int, layer; α_init::Float32=0.01f0)\n",
    "    nC = n * C  # Fixed: consistent variable name\n",
    "    \n",
    "    HyperConnectionLayer(\n",
    "        n, C, layer,\n",
    "        Dense(nC => n),\n",
    "        Dense(nC => n),\n",
    "        Dense(nC => n*n),\n",
    "        zeros(Float32, 1, n),\n",
    "        zeros(Float32, 1, n),\n",
    "        zeros(Float32, n, n),\n",
    "        [α_init],\n",
    "        [α_init],\n",
    "        [α_init]\n",
    "    )\n",
    "end\n",
    "\n",
    "function (hc::HyperConnectionLayer)(x_expanded)\n",
    "    n, C, batch = size(x_expanded)\n",
    "    x_flat = reshape(x_expanded, n * C, batch)\n",
    "    \n",
    "    # Dynamic mappings\n",
    "    H_pre = hc.α_pre[1] .* hc.ϕ_pre(x_flat)' .+ hc.b_pre\n",
    "    H_post = hc.α_post[1] .* hc.ϕ_post(x_flat)' .+ hc.b_post\n",
    "    H_res_flat = hc.α_res[1] .* hc.ϕ_res(x_flat)\n",
    "    \n",
    "    # Unconstrained H_res - THIS IS THE PROBLEM\n",
    "    H_res_mean = reshape(mean(H_res_flat, dims=2), n, n) .+ hc.b_res .+ Float32.(I(n))\n",
    "    \n",
    "    # Pre-mapping (functional - no mutation)\n",
    "    # H_pre: (batch, n) → (n, 1, batch) for broadcasting with x: (n, C, batch)\n",
    "    H_pre_rs = reshape(permutedims(H_pre), n, 1, batch)\n",
    "    h_in = dropdims(sum(H_pre_rs .* x_expanded, dims=1), dims=1) # (C, batch)\n",
    "    \n",
    "    # Apply layer\n",
    "    h_out = hc.layer(h_in)\n",
    "    \n",
    "    # Residual mapping: H_res @ x for each batch\n",
    "    # Reshape to 2D, multiply, reshape backward\n",
    "    x_2d = reshape(x_expanded, n, C * batch)\n",
    "    x_res = reshape(H_res_mean * x_2d, n, C, batch)\n",
    "\n",
    "    # Post mapping: distribute h_out to n streams\n",
    "    # H_post: (batch, n) → (n, 1, batch), h_out: (C, batch) -> (1, C, batch)\n",
    "    H_post_rs = reshape(permutedims(H_post), n, 1, batch)\n",
    "    h_out_rs = reshape(h_out, 1, C, batch)\n",
    "    x_post = H_post_rs .* h_out_rs # (n, C, batch)\n",
    "    \n",
    "    return x_res .+ x_post\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 6: Manifold-Constrained HC Layer (Stable)\n",
    "# H_res projected onto Birkhoff polytope via Sinkhorn-Knopp\n",
    "# =============================================================================\n",
    "\n",
    "struct ManifoldHCLayer\n",
    "    n::Int\n",
    "    C::Int\n",
    "    layer::Any  # Fixed: was layer:Any\n",
    "    \n",
    "    ϕ_pre::Any\n",
    "    ϕ_post::Any\n",
    "    ϕ_res::Any\n",
    "    \n",
    "    b_pre::Any\n",
    "    b_post::Any\n",
    "    b_res::Any\n",
    "    \n",
    "    α_pre::Any\n",
    "    α_post::Any\n",
    "    α_res::Any\n",
    "    \n",
    "    sk_iters::Int\n",
    "end\n",
    "\n",
    "Flux.@layer ManifoldHCLayer\n",
    "\n",
    "function ManifoldHCLayer(n::Int, C::Int, layer; α_init::Float32=0.01f0, sk_iters::Int=20)\n",
    "    nC = n * C  # Fixed: consistent variable name\n",
    "    \n",
    "    ManifoldHCLayer(\n",
    "        n, C, layer,\n",
    "        Dense(nC => n),\n",
    "        Dense(nC => n),\n",
    "        Dense(nC => n*n),\n",
    "        zeros(Float32, 1, n),\n",
    "        zeros(Float32, 1, n),\n",
    "        zeros(Float32, n, n),\n",
    "        [α_init],\n",
    "        [α_init],\n",
    "        [α_init],\n",
    "        sk_iters\n",
    "    )\n",
    "end\n",
    "\n",
    "function (mhc::ManifoldHCLayer)(x_expanded)\n",
    "    n, C, batch = size(x_expanded)\n",
    "    x_flat = reshape(x_expanded, n * C, batch)\n",
    "    \n",
    "    # Raw mappings\n",
    "    H_pre_raw = mhc.α_pre[1] .* mhc.ϕ_pre(x_flat)' .+ mhc.b_pre\n",
    "    H_post_raw = mhc.α_post[1] .* mhc.ϕ_post(x_flat)' .+ mhc.b_post\n",
    "    H_res_flat = mhc.α_res[1] .* mhc.ϕ_res(x_flat)\n",
    "    \n",
    "    # === MANIFOLD CONSTRAINTS ===\n",
    "    H_pre = sigmoid.(H_pre_raw)              # Non-negative [0, 1]\n",
    "    H_post = 2f0 .* sigmoid.(H_post_raw)     # Non-negative [0, 2]\n",
    "    \n",
    "    # Doubly stochastic via Sinkhorn-Knopp\n",
    "    H_res_mean = reshape(mean(H_res_flat, dims=2), n, n) .+ mhc.b_res\n",
    "    H_res = sinkhorn_knopp(H_res_mean; max_iters=mhc.sk_iters)\n",
    "    \n",
    "    # Pre-mapping\n",
    "    # h_in = zeros(Float32, C, batch)\n",
    "    # for b in 1:batch\n",
    "    #     h_in[:, b] = sum(H_pre[b, i] .* x_expanded[i, :, b] for i in 1:n)\n",
    "    # end\n",
    "\n",
    "    # Pre-mapping (functional)\n",
    "    H_pre_rs = reshape(permutedims(H_pre), n, 1, batch)\n",
    "    h_in = dropdims(sum(H_pre_rs .* x_expanded, dims=1), dims=1)\n",
    "\n",
    "    # Apply layer\n",
    "    h_out = mhc.layer(h_in)\n",
    "    \n",
    "    # Constrained residual and post-mapping\n",
    "    # x_next = similar(x_expanded)\n",
    "    # for b in 1:batch\n",
    "    #     for i in 1:n\n",
    "    #         x_next[i, :, b] = sum(H_res[i, j] .* x_expanded[j, :, b] for j in 1:n)\n",
    "    #     end\n",
    "    #     for i in 1:n\n",
    "    #         x_next[i, :, b] .+= H_post[b, i] .* h_out[:, b]\n",
    "    #     end\n",
    "    # end\n",
    "\n",
    "    # Residual mapping (functional)\n",
    "    x_2d = reshape(x_expanded, n, C * batch)\n",
    "    x_res = reshape(H_res * x_2d, n, C, batch)\n",
    "\n",
    "    # Post mapping (functional)\n",
    "    H_post_rs = reshape(permutedims(H_post), n, 1, batch)\n",
    "    h_out_rs = reshape(h_out, 1, C, batch)\n",
    "    x_post = H_post_rs .* h_out_rs\n",
    "    \n",
    "    return x_res .+ x_post\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 7: Full MLP Networks\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard Residual MLP ---\n",
    "struct ResidualMLP\n",
    "    input_proj::Any\n",
    "    blocks::Any\n",
    "    output_proj::Any\n",
    "    norm::Any\n",
    "end\n",
    "\n",
    "Flux.@layer ResidualMLP\n",
    "\n",
    "function ResidualMLP(input_dim::Int, hidden_dim::Int, output_dim::Int, num_layers::Int)\n",
    "    make_block() = ResidualBlock(\n",
    "        Chain(\n",
    "            LayerNorm(hidden_dim),\n",
    "            Dense(hidden_dim => hidden_dim * 4, relu),\n",
    "            Dense(hidden_dim * 4 => hidden_dim)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ResidualMLP(\n",
    "        Dense(input_dim => hidden_dim),\n",
    "        [make_block() for _ in 1:num_layers],\n",
    "        Dense(hidden_dim => output_dim),\n",
    "        LayerNorm(hidden_dim)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (m::ResidualMLP)(x)\n",
    "    h = m.input_proj(x)\n",
    "    for block in m.blocks\n",
    "        h = block(h)\n",
    "    end\n",
    "    h = m.norm(h)\n",
    "    return m.output_proj(h)\n",
    "end\n",
    "\n",
    "# --- Hyper-Connections MLP ---\n",
    "struct HCMLP\n",
    "    n::Int\n",
    "    input_proj::Any\n",
    "    blocks::Any\n",
    "    output_proj::Any\n",
    "end\n",
    "\n",
    "Flux.@layer HCMLP\n",
    "\n",
    "function HCMLP(input_dim::Int, hidden_dim::Int, output_dim::Int, num_layers::Int; n::Int=4)\n",
    "    make_inner() = Chain(\n",
    "        LayerNorm(hidden_dim),\n",
    "        Dense(hidden_dim => hidden_dim * 4, relu),\n",
    "        Dense(hidden_dim * 4 => hidden_dim)\n",
    "    )\n",
    "    \n",
    "    HCMLP(\n",
    "        n,\n",
    "        Dense(input_dim => hidden_dim * n),\n",
    "        [HyperConnectionLayer(n, hidden_dim, make_inner()) for _ in 1:num_layers],\n",
    "        Chain(\n",
    "            x -> mean(x, dims=1)[1, :, :],\n",
    "            LayerNorm(hidden_dim),\n",
    "            Dense(hidden_dim => output_dim)\n",
    "        )\n",
    "    )\n",
    "end\n",
    "\n",
    "function (m::HCMLP)(x)\n",
    "    batch = size(x, 2)\n",
    "    h_flat = m.input_proj(x)\n",
    "    h = reshape(h_flat, m.n, size(h_flat, 1) ÷ m.n, batch)\n",
    "    for block in m.blocks\n",
    "        h = block(h)\n",
    "    end\n",
    "    return m.output_proj(h)\n",
    "end\n",
    "\n",
    "# --- Manifold-Constrained HC MLP ---\n",
    "struct mHCMLP\n",
    "    n::Int\n",
    "    input_proj::Any\n",
    "    blocks::Any\n",
    "    output_proj::Any\n",
    "end\n",
    "\n",
    "Flux.@layer mHCMLP\n",
    "\n",
    "function mHCMLP(input_dim::Int, hidden_dim::Int, output_dim::Int, num_layers::Int; n::Int=4, sk_iters::Int=20)\n",
    "    make_inner() = Chain(\n",
    "        LayerNorm(hidden_dim),\n",
    "        Dense(hidden_dim => hidden_dim * 4, relu),\n",
    "        Dense(hidden_dim * 4 => hidden_dim)\n",
    "    )\n",
    "    \n",
    "    mHCMLP(\n",
    "        n,\n",
    "        Dense(input_dim => hidden_dim * n),\n",
    "        [ManifoldHCLayer(n, hidden_dim, make_inner(); sk_iters=sk_iters) for _ in 1:num_layers],\n",
    "        Chain(\n",
    "            x -> mean(x, dims=1)[1, :, :],\n",
    "            LayerNorm(hidden_dim),\n",
    "            Dense(hidden_dim => output_dim)\n",
    "        )\n",
    "    )\n",
    "end\n",
    "\n",
    "function (m::mHCMLP)(x)\n",
    "    batch = size(x, 2)\n",
    "    h_flat = m.input_proj(x)\n",
    "    h = reshape(h_flat, m.n, size(h_flat, 1) ÷ m.n, batch)\n",
    "    for block in m.blocks\n",
    "        h = block(h)\n",
    "    end\n",
    "    return m.output_proj(h)\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 8: Training Utilities\n",
    "# =============================================================================\n",
    "\n",
    "function ce_loss(model, x, y)\n",
    "    logits = model(x)\n",
    "    return Flux.logitcrossentropy(logits, y)\n",
    "end\n",
    "\n",
    "function accuracy(model, x, y)\n",
    "    logits = model(x)\n",
    "    preds = onecold(logits)\n",
    "    targets = onecold(y)\n",
    "    return mean(preds .== targets)\n",
    "end\n",
    "\n",
    "function compute_grad_norm(grads)\n",
    "    total = 0.0f0\n",
    "    Θ, _ = destructure(grads)\n",
    "    return sqrt(sum(abs2, Θ))\n",
    "end\n",
    "\n",
    "function count_params(model)\n",
    "    Θ, _ = destructure(model)\n",
    "    return length(Θ)\n",
    "end\n",
    "\n",
    "struct TrainingHistory\n",
    "    losses::Vector{Float32}\n",
    "    accuracies::Vector{Float32}\n",
    "    grad_norms::Vector{Float32}\n",
    "    forward_gains::Vector{Float32}\n",
    "    backward_gains::Vector{Float32}\n",
    "end\n",
    "\n",
    "TrainingHistory() = TrainingHistory(Float32[], Float32[], Float32[], Float32[], Float32[])\n",
    "\n",
    "function extract_hres_matrices(model::mHCMLP)\n",
    "    matrices = Matrix{Float32}[]\n",
    "    for block in model.blocks\n",
    "        n, C = block.n, block.C\n",
    "        dummy = zeros(Float32, n * C, 1)\n",
    "        H_res_flat = block.α_res[1] .* block.ϕ_res(dummy)\n",
    "        H_res_raw = reshape(H_res_flat, n, n) .+ block.b_res\n",
    "        H_res = sinkhorn_knopp(H_res_raw; max_iters=block.sk_iters)\n",
    "        push!(matrices, H_res)\n",
    "    end\n",
    "    return matrices\n",
    "end\n",
    "\n",
    "function extract_hres_matrices(model::HCMLP)\n",
    "    matrices = Matrix{Float32}[]\n",
    "    for block in model.blocks\n",
    "        n, C = block.n, block.C\n",
    "        dummy = zeros(Float32, n * C, 1)\n",
    "        H_res_flat = block.α_res[1] .* block.ϕ_res(dummy)\n",
    "        H_res = reshape(H_res_flat, n, n) .+ block.b_res .+ I(n)\n",
    "        push!(matrices, H_res)\n",
    "    end\n",
    "    return matrices\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 9: Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "function train_epoch!(model, opt_state, train_x, train_y, history::TrainingHistory)\n",
    "    batch_size = 64\n",
    "    n_samples = size(train_x, 2)\n",
    "    n_batches = n_samples ÷ batch_size\n",
    "    \n",
    "    epoch_loss = 0.0f0\n",
    "    epoch_grad_norm = 0.0f0\n",
    "    \n",
    "    for i in 1:n_batches\n",
    "        idx = ((i-1)*batch_size + 1):(i*batch_size)\n",
    "        x_batch = train_x[:, idx]\n",
    "        y_batch = train_y[:, idx]\n",
    "        \n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            ce_loss(m, x_batch, y_batch)\n",
    "        end\n",
    "        \n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_grad_norm += compute_grad_norm(grads[1])\n",
    "    end\n",
    "    \n",
    "    push!(history.losses, epoch_loss / n_batches)\n",
    "    push!(history.grad_norms, epoch_grad_norm / n_batches)\n",
    "    push!(history.accuracies, accuracy(model, train_x, train_y))\n",
    "    \n",
    "    # Stability metrics\n",
    "    if model isa HCMLP || model isa mHCMLP\n",
    "        matrices = extract_hres_matrices(model)\n",
    "        if !isempty(matrices)\n",
    "            gain = measure_composite_gain(matrices)\n",
    "            push!(history.forward_gains, gain.forward)\n",
    "            push!(history.backward_gains, gain.backward)\n",
    "        end\n",
    "    else\n",
    "        push!(history.forward_gains, 1.0f0)\n",
    "        push!(history.backward_gains, 1.0f0)\n",
    "    end\n",
    "    \n",
    "    return history\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 10: Data Generation\n",
    "# =============================================================================\n",
    "\n",
    "function generate_synthetic_data(n_samples::Int, input_dim::Int, n_classes::Int; seed::Int=42)\n",
    "    Random.seed!(seed)\n",
    "    \n",
    "    centers = randn(Float32, input_dim, n_classes) .* 3\n",
    "    samples_per_class = n_samples ÷ n_classes\n",
    "    \n",
    "    X = zeros(Float32, input_dim, n_samples)\n",
    "    Y = zeros(Float32, n_classes, n_samples)\n",
    "    \n",
    "    for c in 1:n_classes\n",
    "        idx_start = (c-1) * samples_per_class + 1\n",
    "        idx_end = c * samples_per_class\n",
    "        X[:, idx_start:idx_end] = centers[:, c] .+ randn(Float32, input_dim, samples_per_class) .* 0.5\n",
    "        Y[c, idx_start:idx_end] .= 1.0f0\n",
    "    end\n",
    "    \n",
    "    perm = randperm(n_samples)\n",
    "    return X[:, perm], Y[:, perm]\n",
    "end\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 11: Quick Demo - Sinkhorn-Knopp\n",
    "# Run this to understand the core algorithm\n",
    "# =============================================================================\n",
    "\n",
    "function demo_sinkhorn()\n",
    "    println(\"=\"^50)\n",
    "    println(\"DEMO: Sinkhorn-Knopp Projection\")\n",
    "    println(\"=\"^50)\n",
    "    \n",
    "    # Random 4x4 matrix\n",
    "    M = randn(Float32, 4, 4)\n",
    "    println(\"\\nRandom matrix:\")\n",
    "    display(round.(M, digits=2))\n",
    "    \n",
    "    # Project to doubly stochastic\n",
    "    P = sinkhorn_knopp(M)\n",
    "    println(\"\\n\\nAfter Sinkhorn-Knopp (doubly stochastic):\")\n",
    "    display(round.(P, digits=3))\n",
    "    \n",
    "    println(\"\\nVerification:\")\n",
    "    check_doubly_stochastic(P)\n",
    "end\n",
    "\n",
    "# Uncomment to run:\n",
    "# demo_sinkhorn()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 12: Demo - Propagation Stability\n",
    "# Shows why mHC is stable and HC is not\n",
    "# =============================================================================\n",
    "\n",
    "function demo_stability()\n",
    "    println(\"=\"^50)\n",
    "    println(\"DEMO: Propagation Stability Comparison\")\n",
    "    println(\"=\"^50)\n",
    "    \n",
    "    n = 4\n",
    "    num_layers = 30\n",
    "    \n",
    "    # Unconstrained (HC)\n",
    "    println(\"\\n--- Unconstrained (HC-style) ---\")\n",
    "    hc_matrices = [randn(Float32, n, n) for _ in 1:num_layers]\n",
    "    \n",
    "    single = measure_gain(hc_matrices[1])\n",
    "    println(\"Single layer: forward=$(round(single.forward, digits=2)), backward=$(round(single.backward, digits=2))\")\n",
    "    \n",
    "    composite = measure_composite_gain(hc_matrices)\n",
    "    println(\"30-layer composite: forward=$(round(composite.forward, digits=2)), backward=$(round(composite.backward, digits=2))\")\n",
    "    \n",
    "    # Doubly stochastic (mHC)\n",
    "    println(\"\\n--- Doubly Stochastic (mHC-style) ---\")\n",
    "    mhc_matrices = [sinkhorn_knopp(randn(Float32, n, n)) for _ in 1:num_layers]\n",
    "    \n",
    "    single = measure_gain(mhc_matrices[1])\n",
    "    println(\"Single layer: forward=$(round(single.forward, digits=2)), backward=$(round(single.backward, digits=2))\")\n",
    "    \n",
    "    composite = measure_composite_gain(mhc_matrices)\n",
    "    println(\"30-layer composite: forward=$(round(composite.forward, digits=2)), backward=$(round(composite.backward, digits=2))\")\n",
    "    \n",
    "    return hc_matrices, mhc_matrices\n",
    "end\n",
    "\n",
    "# Uncomment to run:\n",
    "# demo_stability()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 13: Experiment - Full Training Comparison\n",
    "# =============================================================================\n",
    "\n",
    "function run_training_comparison(;\n",
    "    input_dim::Int=32,\n",
    "    hidden_dim::Int=64,\n",
    "    output_dim::Int=10,\n",
    "    num_layers::Int=8,\n",
    "    n_samples::Int=3000,\n",
    "    n_epochs::Int=30,\n",
    "    lr::Float64=1e-3,\n",
    "    expansion::Int=4\n",
    ")\n",
    "    println(\"=\"^60)\n",
    "    println(\"TRAINING COMPARISON: Residual vs HC vs mHC\")\n",
    "    println(\"=\"^60)\n",
    "    println(\"Layers: $num_layers, Hidden: $hidden_dim, Expansion: $expansion\")\n",
    "    \n",
    "    # Data\n",
    "    train_x, train_y = generate_synthetic_data(n_samples, input_dim, output_dim)\n",
    "    println(\"Data: $(size(train_x))\")\n",
    "    \n",
    "    # Models\n",
    "    println(\"\\nCreating models...\")\n",
    "    residual = ResidualMLP(input_dim, hidden_dim, output_dim, num_layers)\n",
    "    hc = HCMLP(input_dim, hidden_dim, output_dim, num_layers; n=expansion)\n",
    "    mhc = mHCMLP(input_dim, hidden_dim, output_dim, num_layers; n=expansion)\n",
    "    \n",
    "    println(\"ResidualMLP: $(count_params(residual)) params\")\n",
    "    println(\"HCMLP: $(count_params(hc)) params\")\n",
    "    println(\"mHCMLP: $(count_params(mhc)) params\")\n",
    "    \n",
    "    # Training\n",
    "    hist_res = TrainingHistory()\n",
    "    hist_hc = TrainingHistory()\n",
    "    hist_mhc = TrainingHistory()\n",
    "    \n",
    "    opt_res = Flux.setup(Adam(lr), residual)\n",
    "    opt_hc = Flux.setup(Adam(lr), hc)\n",
    "    opt_mhc = Flux.setup(Adam(lr), mhc)\n",
    "    \n",
    "    println(\"\\nTraining...\")\n",
    "    for epoch in 1:n_epochs\n",
    "        train_epoch!(residual, opt_res, train_x, train_y, hist_res)\n",
    "        train_epoch!(hc, opt_hc, train_x, train_y, hist_hc)\n",
    "        train_epoch!(mhc, opt_mhc, train_x, train_y, hist_mhc)\n",
    "        \n",
    "        if epoch % 5 == 0 || epoch == 1\n",
    "            println(\"Epoch $epoch:\")\n",
    "            println(\"  Res - Loss: $(round(hist_res.losses[end], digits=4)), Acc: $(round(hist_res.accuracies[end]*100, digits=1))%\")\n",
    "            println(\"  HC  - Loss: $(round(hist_hc.losses[end], digits=4)), Acc: $(round(hist_hc.accuracies[end]*100, digits=1))%, Gain: $(round(hist_hc.forward_gains[end], digits=2))\")\n",
    "            println(\"  mHC - Loss: $(round(hist_mhc.losses[end], digits=4)), Acc: $(round(hist_mhc.accuracies[end]*100, digits=1))%, Gain: $(round(hist_mhc.forward_gains[end], digits=2))\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (\n",
    "        models = (residual=residual, hc=hc, mhc=mhc),\n",
    "        histories = (residual=hist_res, hc=hist_hc, mhc=hist_mhc)\n",
    "    )\n",
    "end\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = run_training_comparison(n_epochs=30)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 14: Visualisation - Propagation Stability\n",
    "# =============================================================================\n",
    "\n",
    "function plot_propagation_stability(; n::Int=4, num_layers::Int=60)\n",
    "    hc_forward, hc_backward = Float32[], Float32[]\n",
    "    mhc_forward, mhc_backward = Float32[], Float32[]\n",
    "    \n",
    "    hc_matrices = [randn(Float32, n, n) .* 0.5f0 .+ I(n) for _ in 1:num_layers]\n",
    "    mhc_matrices = [sinkhorn_knopp(randn(Float32, n, n)) for _ in 1:num_layers]\n",
    "    \n",
    "    for l in 1:num_layers\n",
    "        hc_comp = reduce(*, hc_matrices[1:l])\n",
    "        mhc_comp = reduce(*, mhc_matrices[1:l])\n",
    "        \n",
    "        hc_g = measure_gain(hc_comp)\n",
    "        mhc_g = measure_gain(mhc_comp)\n",
    "        \n",
    "        push!(hc_forward, hc_g.forward)\n",
    "        push!(mhc_forward, mhc_g.forward)\n",
    "        push!(hc_backward, hc_g.backward)\n",
    "        push!(mhc_backward, mhc_g.backward)\n",
    "    end\n",
    "    \n",
    "    p1 = plot(1:num_layers, hc_forward, label=\"HC\", yscale=:log10, \n",
    "              ylabel=\"Gain\", xlabel=\"Depth\", title=\"Forward Signal Gain\",\n",
    "              linewidth=2, color=:red)\n",
    "    plot!(p1, 1:num_layers, mhc_forward, label=\"mHC\", linewidth=2, color=:blue)\n",
    "    hline!(p1, [1.0], label=\"Ideal\", linestyle=:dash, color=:black)\n",
    "    \n",
    "    p2 = plot(1:num_layers, hc_backward, label=\"HC\", yscale=:log10,\n",
    "              ylabel=\"Gain\", xlabel=\"Depth\", title=\"Backward Gradient Gain\",\n",
    "              linewidth=2, color=:red)\n",
    "    plot!(p2, 1:num_layers, mhc_backward, label=\"mHC\", linewidth=2, color=:blue)\n",
    "    hline!(p2, [1.0], label=\"Ideal\", linestyle=:dash, color=:black)\n",
    "    \n",
    "    plot(p1, p2, layout=(1,2), size=(900, 400))\n",
    "end\n",
    "\n",
    "# Uncomment to run:\n",
    "# plot_propagation_stability()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 15: Visualisation - Training Dynamics\n",
    "# =============================================================================\n",
    "\n",
    "function plot_training_results(results)\n",
    "    h_res = results.histories.residual\n",
    "    h_hc = results.histories.hc\n",
    "    h_mhc = results.histories.mhc\n",
    "    \n",
    "    p1 = plot(h_res.losses, label=\"Residual\", ylabel=\"Loss\", xlabel=\"Epoch\", \n",
    "              title=\"Training Loss\", linewidth=2)\n",
    "    plot!(p1, h_hc.losses, label=\"HC\", linewidth=2)\n",
    "    plot!(p1, h_mhc.losses, label=\"mHC\", linewidth=2)\n",
    "    \n",
    "    p2 = plot(h_res.grad_norms, label=\"Residual\", ylabel=\"Grad Norm\", xlabel=\"Epoch\",\n",
    "              title=\"Gradient Norm\", linewidth=2)\n",
    "    plot!(p2, h_hc.grad_norms, label=\"HC\", linewidth=2)\n",
    "    plot!(p2, h_mhc.grad_norms, label=\"mHC\", linewidth=2)\n",
    "    \n",
    "    p3 = plot(h_res.accuracies .* 100, label=\"Residual\", ylabel=\"Acc %\", xlabel=\"Epoch\",\n",
    "              title=\"Training Accuracy\", linewidth=2)\n",
    "    plot!(p3, h_hc.accuracies .* 100, label=\"HC\", linewidth=2)\n",
    "    plot!(p3, h_mhc.accuracies .* 100, label=\"mHC\", linewidth=2)\n",
    "    \n",
    "    p4 = plot(h_hc.forward_gains, label=\"HC\", ylabel=\"Gain\", xlabel=\"Epoch\",\n",
    "              title=\"Signal Gain (mHC ≈ 1)\", linewidth=2, color=:red)\n",
    "    plot!(p4, h_mhc.forward_gains, label=\"mHC\", linewidth=2, color=:blue)\n",
    "    hline!(p4, [1.0], label=\"Ideal\", linestyle=:dash, color=:black)\n",
    "    \n",
    "    plot(p1, p2, p3, p4, layout=(2,2), size=(900, 700))\n",
    "end\n",
    "\n",
    "# Uncomment after running training:\n",
    "# plot_training_results(results)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 16: Visualisation - Sinkhorn Convergence\n",
    "# =============================================================================\n",
    "\n",
    "function plot_sinkhorn_convergence(; n::Int=4, max_iters::Int=50)\n",
    "    M = randn(Float32, n, n)\n",
    "    P = exp.(M)\n",
    "    \n",
    "    row_err, col_err = Float32[], Float32[]\n",
    "    \n",
    "    for _ in 1:max_iters\n",
    "        P = P ./ sum(P, dims=2)\n",
    "        P = P ./ sum(P, dims=1)\n",
    "        push!(row_err, maximum(abs.(sum(P, dims=2) .- 1)))\n",
    "        push!(col_err, maximum(abs.(sum(P, dims=1) .- 1)))\n",
    "    end\n",
    "    \n",
    "    p = plot(1:max_iters, row_err, label=\"Row error\", yscale=:log10,\n",
    "             xlabel=\"Iteration\", ylabel=\"Max Error\", \n",
    "             title=\"Sinkhorn-Knopp Convergence\", linewidth=2)\n",
    "    plot!(p, 1:max_iters, col_err, label=\"Col error\", linewidth=2)\n",
    "    vline!(p, [20], label=\"Default (20)\", linestyle=:dash, color=:black)\n",
    "    \n",
    "    return p\n",
    "end\n",
    "\n",
    "# Uncomment to run:\n",
    "# plot_sinkhorn_convergence()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 17: Run Everything\n",
    "# =============================================================================\n",
    "\n",
    "function run_all()\n",
    "    println(\"\\n\" * \"=\"^60)\n",
    "    println(\"mHC: Complete Demonstration\")\n",
    "    println(\"=\"^60)\n",
    "    \n",
    "    # 1. Sinkhorn demo\n",
    "    demo_sinkhorn()\n",
    "    \n",
    "    # 2. Stability demo\n",
    "    demo_stability()\n",
    "    \n",
    "    # 3. Training comparison\n",
    "    results = run_training_comparison(n_epochs=25)\n",
    "    \n",
    "    # 4. Plots\n",
    "    display(plot_propagation_stability())\n",
    "    display(plot_training_results(results))\n",
    "    display(plot_sinkhorn_convergence())\n",
    "    \n",
    "    return results\n",
    "end\n",
    "\n",
    "# Uncomment to run everything:\n",
    "results = run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.3",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
